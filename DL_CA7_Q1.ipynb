{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "DL_CA7_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30e5222b4fc6434ea02d54811c2f3ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b8f80f3557b48c3b1d2a994913bee7a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a1ba092a69b4677a81489652021cb5b",
              "IPY_MODEL_7fc421ce4ef047cd8ee3d6e507fe3289"
            ]
          }
        },
        "3b8f80f3557b48c3b1d2a994913bee7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a1ba092a69b4677a81489652021cb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9be8c57cb12491f82a20abee3f8701d",
            "_dom_classes": [],
            "description": "Epoch: 1:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 228,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ac6ebc8e28244ae8e804b39f349053b"
          }
        },
        "7fc421ce4ef047cd8ee3d6e507fe3289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66c52553d7d144d58aeb192f7f5dbc9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/228 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1721908040824e8c9736149dbfae557e"
          }
        },
        "a9be8c57cb12491f82a20abee3f8701d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ac6ebc8e28244ae8e804b39f349053b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66c52553d7d144d58aeb192f7f5dbc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1721908040824e8c9736149dbfae557e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeryET/DeepLearning_CA7/blob/master/DL_CA7_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYnxtVCLosyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e757857-d7f1-4606-d66b-fcd7ef1cdc25"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/kaggle\"\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip -qo \"/content/flickr8k.zip\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gIpON9soyqp"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re\n",
        "import string\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import string \n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ZtWLvWDgAI"
      },
      "source": [
        "# Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFHNnE8-Dh-n"
      },
      "source": [
        "# Locations\n",
        "IMAGE_ROOT = \"/content/Images\"\n",
        "CAPTION_CSV_LOC = \"/content/captions.txt\"\n",
        "\n",
        "\n",
        "# Tokens\n",
        "START_TOKEN = \"<SOS>\"\n",
        "END_TOKEN = \"<EOS>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "# UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "# Captions related\n",
        "MAX_LENGTH = 256 # LSTM units\n",
        "MIN_WORD_FREQ = 15\n",
        "EMBEDDING_DIM = 100\n",
        "CAPTIONS_PER_IMAGE = 5\n",
        "\n",
        "\n",
        "# Dataset specifice\n",
        "TEST_SPLIT = 0.1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rDlvc4rSEr"
      },
      "source": [
        "# Processing Documents\n",
        "\n",
        "### Preprocessing Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ihCQ0a4rUso"
      },
      "source": [
        "def preprocess_text(text):\n",
        "  prep = text.lower()\n",
        "  prep = re.sub(\"\\s+\", \" \", prep)\n",
        "  prep = prep.translate(str.maketrans('', '', string.punctuation))\n",
        "  return prep.split()\n",
        "\n",
        "\n",
        "caption_df = pd.read_csv(CAPTION_CSV_LOC)\n",
        "caption_df.sort_values(by=\"image\", inplace=True)\n",
        "caption_df[\"cleaned\"] = caption_df[\"caption\"].apply(preprocess_text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YufKpMZJA1ED"
      },
      "source": [
        "### Creating Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_daAgQdTA4Tn"
      },
      "source": [
        "counter = collections.Counter(itertools.chain(*caption_df[\"cleaned\"]))\n",
        "words = sorted([v for v, n in counter.items() if n > MIN_WORD_FREQ])\n",
        "words += [PAD_TOKEN, START_TOKEN, END_TOKEN] + words\n",
        "vocab = {w: idx for idx, w in enumerate(words)}\n",
        "id2word = {idx: w for idx, w in enumerate(words)}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8rIEe6fBqfX"
      },
      "source": [
        "### Defining Transforms for Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAIlqnscAdu8"
      },
      "source": [
        "class VocabTransform:\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def __call__(self, tokenized):\n",
        "    return [self.vocab[t] for t in tokenized if t in self.vocab.keys()]\n",
        "\n",
        "class CaptionConditioner:\n",
        "  def __init__(self, max_length=MAX_LENGTH):\n",
        "    self.max_length = max_length\n",
        "  \n",
        "  def __call__(self, tokenized):\n",
        "    return [START_TOKEN] + tokenized[:self.max_length - 2] + [END_TOKEN]\n",
        "\n",
        "class TextIndicesToTensor:\n",
        "  def __call__(self, item):\n",
        "    return torch.tensor(tuple(item), dtype=torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "caption_transforms = transforms.Compose(\n",
        "    [\n",
        "     CaptionConditioner(),\n",
        "     VocabTransform(vocab),\n",
        "     TextIndicesToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htHAhlyas5kL"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JguTXlmetLGk"
      },
      "source": [
        "# This is copied from https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224), # As an augmenting process\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1jrIOfs7dq"
      },
      "source": [
        "# Defining the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49UVBXvFoyqw"
      },
      "source": [
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self, \n",
        "               images_path, \n",
        "               caption_df, \n",
        "               image_transforms, \n",
        "               caption_transforms,\n",
        "               captions_per_image=CAPTIONS_PER_IMAGE):\n",
        "    self.image_transforms = image_transforms\n",
        "    self.caption_transforms = caption_transforms\n",
        "    self.images_path = images_path\n",
        "    self.images_fnames = sorted(os.listdir(images_path))\n",
        "    self.df = caption_df\n",
        "    self.captions_per_image = captions_per_image\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_fnames)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    fname = self.images_fnames[idx]\n",
        "    fpath = os.path.join(self.images_path, fname)\n",
        "    image = Image.open(fpath)\n",
        "    \n",
        "    idx *= self.captions_per_image # num of repeats\n",
        "    captions = list(self.df.iloc[idx:idx+self.captions_per_image][\"cleaned\"])\n",
        "\n",
        "    image = self.image_transforms(image)\n",
        "    captions = [self.caption_transforms(c) for c in captions]\n",
        "\n",
        "    return image, captions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRquY0WnC7jU"
      },
      "source": [
        "\n",
        "# Influenced by https://github.com/siddsrivastava/Image-captioning/blob/master/model.py\n",
        "\n",
        "class FlickrEncoderCNN(nn.Module):\n",
        "  def __init__(self, \n",
        "               embedding_dim, \n",
        "               freeze):\n",
        "    super().__init__()\n",
        "    # Loading resnet\n",
        "    resnet = torchvision.models.resnet18(pretrained=True)\n",
        "    fc_in_features = resnet.fc.in_features\n",
        "\n",
        "    # Defining layers\n",
        "    modules = list(resnet.children())[:-1]\n",
        "    self.resnet = nn.Sequential(*modules)\n",
        "    self.fc = nn.Linear(fc_in_features, embedding_dim)\n",
        "\n",
        "    # Freezing if needed\n",
        "    if freeze:\n",
        "      for param in self.resnet.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in self.fc.parameters():\n",
        "      param.requires_grad = True\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.resnet(x)\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class FlickrDecoderLSTM(nn.Module):\n",
        "  def __init__(self, \n",
        "                vocab_size,\n",
        "                embedding_dim,\n",
        "                hidden_size,\n",
        "                padding_idx,\n",
        "                bidirectional=False,\n",
        "                dropout=0):\n",
        "    \n",
        "    super().__init__()\n",
        "    # Creating embeddings\n",
        "    self.embed = nn.Embedding(num_embeddings=vocab_size,\n",
        "                              embedding_dim=embedding_dim, \n",
        "                              padding_idx=padding_idx,\n",
        "                              scale_grad_by_freq=True,\n",
        "                              sparse=True,\n",
        "                              )\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True,\n",
        "                        dropout=dropout,\n",
        "                        bidirectional=bidirectional)\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "  def forward(self, features, caption_seqs):\n",
        "      caption_seqs = caption_seqs[:,:-1] \n",
        "      embeddings = self.embed(caption_seqs)\n",
        "      total_input = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "      lstm_out, hidden = self.lstm(total_input)\n",
        "      outputs = self.fc(lstm_out)\n",
        "      return outputs, hidden\n",
        "\n",
        "\n",
        "class FlickrNet(nn.Module):\n",
        "  def __init__(self, \n",
        "               vocab_size,\n",
        "               embedding_dim,\n",
        "               hidden_size,\n",
        "               padding_idx,\n",
        "               end_token_index,\n",
        "               bidirectional=False,\n",
        "               dropout=0,\n",
        "               freeze=True):\n",
        "    super().__init__()\n",
        "    encoder = FlickrEncoderCNN(embedding_dim, freeze)\n",
        "    decoder = FlickrDecoderLSTM(vocab_size, \n",
        "                                    embedding_dim, \n",
        "                                    hidden_size, \n",
        "                                    padding_idx, \n",
        "                                    bidirectional, \n",
        "                                    dropout)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.modules = nn.ModuleList([encoder,decoder])\n",
        "    self.end_token_index = end_token_index\n",
        "\n",
        "  def forward(self, images, caption_seqs):\n",
        "    features = self.encoder(images)\n",
        "    outputs = self.decoder(features, caption_seqs)\n",
        "    return outputs\n",
        "  \n",
        "  def predict(self, image, states, max_len = 30):\n",
        "    with torch.no_grad():\n",
        "      # Needs to be turned into 1 x Channels x Width x Height\n",
        "      features = self.encoder(image.unsqueeze(0))\n",
        "      caption_seq = torch.FloatTensor(features.shape[0], 1, 1)\n",
        "      for _ in range(max_len):\n",
        "        outputs, hidden = self.decoder(features, caption_seq)\n",
        "        predicted_index = F.softmax(outputs).argmax()[..., -1,...].unsqueeze(-1)\n",
        "        caption_seq = torch.cat((caption_seq, predicted_index), dim=1)\n",
        "        if predicted_index.item() == self.end_token_index:\n",
        "          break\n",
        "    return caption_seq"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea5dmwVX-bxy"
      },
      "source": [
        "# This is needed for batches\n",
        "\n",
        "class RepeatImages:\n",
        "  def __init__(self, num_repeat=CAPTIONS_PER_IMAGE):\n",
        "    self.num_repeat = num_repeat\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    result = []\n",
        "    for image, captions in batch:\n",
        "      for i in range(self.num_repeat):\n",
        "        result.append((image, captions[i]))\n",
        "    return result\n",
        "\n",
        "class PadCaptions:\n",
        "  def __init__(self, vocab):\n",
        "    self.pad_idx = vocab[PAD_TOKEN]\n",
        "  \n",
        "  def __call__(self, batch):\n",
        "    captions = []\n",
        "    images = []\n",
        "    for im, cap in batch:\n",
        "      captions.append(cap)\n",
        "      images.append(im.unsqueeze(0))\n",
        "    captions = torch.nn.utils.rnn.pad_sequence(captions,\n",
        "                                    batch_first=True,\n",
        "                                    padding_value=self.pad_idx)\n",
        "    images = torch.cat(images, dim=0)\n",
        "    return images, captions\n",
        "\n",
        "\n",
        "batch_transforms = transforms.Compose([\n",
        "     RepeatImages(),\n",
        "     PadCaptions(vocab),\n",
        "     ]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw6i9Hy_EiSu"
      },
      "source": [
        "# Preparing for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxRyH2B0GSx-"
      },
      "source": [
        "### Defining training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIg_4OcEGV1J"
      },
      "source": [
        "def train(model, optimizer, loss_function, train_loader, max_epochs=50):\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  info = []\n",
        "  for epoch in range(1, max_epochs+1):\n",
        "    epoch_loss = 0\n",
        "    n_items = 0\n",
        "    pbar = tqdm(total=len(train_loader), desc=f\"Epoch: {epoch}\")\n",
        "    for batch in train_loader:\n",
        "      torch.cuda.empty_cache() \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      images, captions = batch\n",
        "      images, captions = images.cuda(), captions.cuda()\n",
        "      \n",
        "      with torch.cuda.amp.autocast():\n",
        "        outputs = model(images, captions)\n",
        "        loss = loss_function(captions, outputs)\n",
        "\n",
        "      # scaling the loss\n",
        "      loss = scaler.scale(loss)\n",
        "      loss.backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      n_items = captions.numel() # Batchsize\n",
        "      pbar.set_postfix(batch_loss=f\"{epoch_loss/captions.numel():.3f}\")\n",
        "      pbar.update()\n",
        "    epoch_loss /= n_items()\n",
        "    info.append(\n",
        "        {\"loss\": epoch_loss}\n",
        "    )\n",
        "    print(f\"Epoch: {epoch}, Loss: {epoch_loss:.3f}\")\n",
        "  return info"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0L4XNhlK0cX"
      },
      "source": [
        "### Defining Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwrob3x2K4R9"
      },
      "source": [
        "def test(model, loss_function, test_loader, batch_transforms):\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  test_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images, captions = batch\n",
        "        images, captions = images.cuda(), captions.cuda()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "          outputs = model(images, captions)\n",
        "          loss = loss_function(captions, outputs)\n",
        "        \n",
        "\n",
        "        test_loss += scaler.scale(loss).item()\n",
        "        n_items += captions.numel() # Batchsize\n",
        "        \n",
        "    test_loss /= n_items()\n",
        "  print(f\"Test Loss: {test_loss:.3f}\")\n",
        "  return test_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9BU36BiHWMf"
      },
      "source": [
        "dataset = FlickrDataset(\n",
        "    images_path=IMAGE_ROOT, \n",
        "    caption_df=caption_df,\n",
        "    image_transforms=image_transforms, \n",
        "    caption_transforms=caption_transforms,\n",
        ")\n",
        "test_length = int(TEST_SPLIT * len(dataset))\n",
        "train_length = len(dataset) - test_length\n",
        "train_dataset, test_dataset = random_split(dataset, [train_length, test_length])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1TdgZBIEnCs"
      },
      "source": [
        "train_batch_size = 32\n",
        "test_batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, collate_fn=batch_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=batch_transforms)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn35I9B1MiMo"
      },
      "source": [
        "# Creating models and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrxjwILSMea9"
      },
      "source": [
        "model = FlickrNet(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_size=EMBEDDING_DIM,\n",
        "    padding_idx=vocab[PAD_TOKEN],\n",
        "    end_token_index=vocab[END_TOKEN],           \n",
        "    bidirectional=False,\n",
        "    dropout=0,\n",
        "    freeze=True\n",
        "    ).cuda()\n",
        "loss_function = nn.CrossEntropyLoss(reduction=\"sum\").cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.01)\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "30e5222b4fc6434ea02d54811c2f3ab4",
            "3b8f80f3557b48c3b1d2a994913bee7a",
            "7a1ba092a69b4677a81489652021cb5b",
            "7fc421ce4ef047cd8ee3d6e507fe3289",
            "a9be8c57cb12491f82a20abee3f8701d",
            "3ac6ebc8e28244ae8e804b39f349053b",
            "66c52553d7d144d58aeb192f7f5dbc9d",
            "1721908040824e8c9736149dbfae557e"
          ]
        },
        "id": "_BFGjE_IN9rP",
        "outputId": "eadb4269-5d41-4ded-fdb7-cf4deeb8d58a"
      },
      "source": [
        "train_info = train(model, optimizer, loss_function, train_loader, max_epochs=50)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30e5222b4fc6434ea02d54811c2f3ab4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch: 1', max=228.0, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4aea08510d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-66c0b43f8395>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_function, train_loader, max_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m# scaling the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"host_softmax\" not implemented for 'Long'"
          ]
        }
      ]
    }
  ]
}
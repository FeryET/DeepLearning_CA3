{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "DL_CA7_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeryET/DeepLearning_CA7/blob/master/DL_CA7_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYnxtVCLosyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4429cca7-63eb-41a3-d92e-95c9bcfca67b"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/kaggle\"\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip -qo \"/content/flickr8k.zip\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gIpON9soyqp"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re\n",
        "import string\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import string \n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ZtWLvWDgAI"
      },
      "source": [
        "# Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFHNnE8-Dh-n"
      },
      "source": [
        "START_TOKEN = \"<SOS>\"\n",
        "END_TOKEN = \"<EOS>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "MAX_LENGTH = 150\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rDlvc4rSEr"
      },
      "source": [
        "# Preprocessing Pipelines\n",
        "\n",
        "### Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ihCQ0a4rUso"
      },
      "source": [
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "  prep = text.lower()\n",
        "  prep = re.sub(\"\\s+\", \" \", prep)\n",
        "  prep = prep.translate(string.punctuation)\n",
        "  prep = f\"{START_TOKEN} {prep.strip()} {END_TOKEN}\"\n",
        "  return prep.split()\n",
        "\n",
        "\n",
        "class VocabTransform:\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def __call__(self, tokenized):\n",
        "    return [self.vocab[t] for t in tokenized]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htHAhlyas5kL"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JguTXlmetLGk"
      },
      "source": [
        "# This is copied from https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1jrIOfs7dq"
      },
      "source": [
        "# Defining the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49UVBXvFoyqw"
      },
      "source": [
        "class FilckrDataset(Dataset):\n",
        "  def __init__(self, images_path, csv_path, image_transforms, caption_transforms):\n",
        "    self.image_transforms = image_transforms\n",
        "    self.caption_transforms = caption_transforms\n",
        "    self.images_path = images_path\n",
        "    self.images_fnames = sorted(os.listdir(images_path))\n",
        "    self.csv_path = csv_path\n",
        "    self.df = pd.read_csv(csv_path)\n",
        "    self._preprocess_captions()\n",
        "  \n",
        "  def _preprocess_captions(self):\n",
        "    self.df[\"cleaned\"] = self.df[\"caption\"].apply(preprocess_text)\n",
        "    self.df.sort_values(by=\"image\", inplace=True)\n",
        "    vocab = set(itertools.chain(self.df[\"cleaned\"]))\n",
        "    vocab.add(PAD_TOKEN)\n",
        "    vocab = sorted(vocab)\n",
        "    self.vocab = {v: idx for idx , v in enumerate(self.vocab)}\n",
        "  \n",
        "  @property\n",
        "  def vocab_len(self):\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    self.step = 5\n",
        "    idx *= step # num of repeats\n",
        "    fname = self.images_fnames[idx]\n",
        "    fpath = os.path.join(self.image_path, fname)\n",
        "    \n",
        "    image = Image.open(fpath)\n",
        "    captions = self.df.iloc[idx:idx+step][\"cleaned\"]\n",
        "    # Truncating\n",
        "    captions = [c[:MAX_LENGTH] for c in captions]\n",
        "\n",
        "    image = self.image_transforms(image)\n",
        "    captions = self.caption_transforms(captions)\n",
        "\n",
        "    return {\n",
        "        \"image\": image, \"captions\": captions\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea5dmwVX-bxy"
      },
      "source": [
        "# This is needed for batches\n",
        "\n",
        "class RepeatImages:\n",
        "  def __init__(self, num_repeat):\n",
        "    self.num_repeat = num_repeat\n",
        "\n",
        "  def __call__(self, data):\n",
        "    images = data[\"image\"]\n",
        "    images = torch.repeat_interleave(images, self.num_repeat, dim=0)\n",
        "    data[\"image\"] = images\n",
        "    return data\n",
        "\n",
        "class PadCaptions:\n",
        "  def __init__(self, vocab):\n",
        "    self.pad_idx = vocab[PAD_TOKEN]\n",
        "  \n",
        "  def __call__(self, data):\n",
        "    captions = data[\"captions\"]\n",
        "    torch.nn.utils.rnn.pad_sequence(captions,\n",
        "                                    batch_first=True,\n",
        "                                    padding_vale=self.pad_idx)\n",
        "    data[\"captions\"] = captions\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRquY0WnC7jU"
      },
      "source": [
        "class FlickrRNN(nn.Module):\n",
        "  # Influenced by https://github.com/siddsrivastava/Image-captioning/blob/master/model.py\n",
        "  class EncoderCNN:\n",
        "    def __init__(self, embedding_dim, do_freeze):\n",
        "      # Loading resnet\n",
        "      resnet = torchvision.models.resnet18(pretrained=True)\n",
        "      fc_in_features = resnet.fc.in_features\n",
        "      modules = list(resnet.children())[:-1]\n",
        "      self.resnet = nn.Sequential(*modules)\n",
        "      for param in self.resnet.parameters():\n",
        "        # do not freeze if do_freeze = False\n",
        "        param.requires_grad = not do_freeze\n",
        "      self.fc = nn.Linear(fc_in_features, embedding_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.resnet(x)\n",
        "      x = self.fc(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  class DecoderRNN:\n",
        "    def __init__(self, \n",
        "                 len_vocab,\n",
        "                 embedding_dim,\n",
        "                 hidden_size,\n",
        "                 padding_idx,\n",
        "                 bidirectional=False,\n",
        "                 dropout=0):\n",
        "      \n",
        "      # Creating embeddings\n",
        "      self.embed = nn.Embedding(num_embeddings=len_vocab,\n",
        "                                embedding_dim=embedding_dim, \n",
        "                                padding_idx=padding_idx),\n",
        "                                scale_grad_by_freq=True,\n",
        "                                sparse=True,\n",
        "                                )\n",
        "      self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                          hidden_size=hidden_size,\n",
        "                          batch_first=True,\n",
        "                          dropout=dropout\n",
        "                          bidirectional=bidirectional)\n",
        "      self.fc = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "    def forward(self, caption_seq):\n",
        "        caption_seq = caption_seq[:,:-1] \n",
        "        embeddings = self.embed(caption_seq)\n",
        "        total_input = torch.cat((features.unsqueeze(1),embeddings),1)\n",
        "        lstm_out, self.hidden = self.lstm(total_input)\n",
        "        outputs = self.fc(lstm_out)\n",
        "        return outputs\n",
        "\n",
        "  def __init__(self, \n",
        "               len_vocab,\n",
        "               embedding_dim,\n",
        "               hidden_size,\n",
        "               padding_idx,\n",
        "               bidirectional=False,\n",
        "               dropout=0,\n",
        "               freeze=True):\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9BU36BiHWMf",
        "outputId": "8ce7870a-3e13-43f0-b629-ccb62cf7e71c"
      },
      "source": [
        "print(resnet.fc.out_features)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
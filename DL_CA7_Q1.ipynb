{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "DL_CA7_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeryET/DeepLearning_CA7/blob/master/DL_CA7_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYnxtVCLosyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4429cca7-63eb-41a3-d92e-95c9bcfca67b"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/kaggle\"\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip -qo \"/content/flickr8k.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gIpON9soyqp"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re\n",
        "import string\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import string \n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ZtWLvWDgAI"
      },
      "source": [
        "# Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFHNnE8-Dh-n"
      },
      "source": [
        "START_TOKEN = \"<SOS>\"\n",
        "END_TOKEN = \"<EOS>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "# UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "MAX_LENGTH = 150\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rDlvc4rSEr"
      },
      "source": [
        "# Preprocessing Pipelines\n",
        "\n",
        "### Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ihCQ0a4rUso"
      },
      "source": [
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "  prep = text.lower()\n",
        "  prep = re.sub(\"\\s+\", \" \", prep)\n",
        "  prep = prep.translate(string.punctuation)\n",
        "  prep = f\"{START_TOKEN} {prep.strip()} {END_TOKEN}\"\n",
        "  return prep.split()\n",
        "\n",
        "\n",
        "class VocabTransform:\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def __call__(self, tokenized):\n",
        "    return [self.vocab[t] for t in tokenized]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htHAhlyas5kL"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JguTXlmetLGk"
      },
      "source": [
        "# This is copied from https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1jrIOfs7dq"
      },
      "source": [
        "# Defining the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49UVBXvFoyqw"
      },
      "source": [
        "class FilckrDataset(Dataset):\n",
        "  def __init__(self, \n",
        "               images_path, \n",
        "               csv_path, \n",
        "               image_transforms, \n",
        "               caption_transforms, \n",
        "               vocab_thresh=5):\n",
        "    self.image_transforms = image_transforms\n",
        "    self.caption_transforms = caption_transforms\n",
        "    self.images_path = images_path\n",
        "    self.images_fnames = sorted(os.listdir(images_path))\n",
        "    self.csv_path = csv_path\n",
        "    self.df = pd.read_csv(csv_path)\n",
        "    self._preprocess_captions(vocab_thresh)\n",
        "  \n",
        "  def _preprocess_captions(self, vocab_thresh):\n",
        "    self.df[\"cleaned\"] = self.df[\"caption\"].apply(preprocess_text)\n",
        "    self.df.sort_values(by=\"image\", inplace=True)\n",
        "    counter = collections.Counter(itertools.chain(self.df[\"cleaned\"]))\n",
        "    vocab = set(\n",
        "        [k for k, v in counter.items() if v > vocab_thresh]\n",
        "    )\n",
        "    vocab.add(PAD_TOKEN)\n",
        "    vocab = sorted(vocab)\n",
        "    self.vocab = {v: idx for idx , v in enumerate(vocab)}\n",
        "  \n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    self.step = 5\n",
        "    idx *= step # num of repeats\n",
        "    fname = self.images_fnames[idx]\n",
        "    fpath = os.path.join(self.image_path, fname)\n",
        "    \n",
        "    image = Image.open(fpath)\n",
        "    captions = self.df.iloc[idx:idx+step][\"cleaned\"]\n",
        "    # Truncating\n",
        "    captions = [c[:MAX_LENGTH] for c in captions]\n",
        "\n",
        "    image = self.image_transforms(image)\n",
        "    captions = self.caption_transforms(captions)\n",
        "\n",
        "    return {\n",
        "        \"image\": image, \"captions\": captions\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea5dmwVX-bxy"
      },
      "source": [
        "# This is needed for batches\n",
        "\n",
        "class RepeatImages:\n",
        "  def __init__(self, num_repeat):\n",
        "    self.num_repeat = num_repeat\n",
        "\n",
        "  def __call__(self, data):\n",
        "    images = data[\"image\"]\n",
        "    images = torch.repeat_interleave(images, self.num_repeat, dim=0)\n",
        "    data[\"image\"] = images\n",
        "    return data\n",
        "\n",
        "class PadCaptions:\n",
        "  def __init__(self, vocab):\n",
        "    self.pad_idx = vocab[PAD_TOKEN]\n",
        "  \n",
        "  def __call__(self, data):\n",
        "    captions = data[\"captions\"]\n",
        "    torch.nn.utils.rnn.pad_sequence(captions,\n",
        "                                    batch_first=True,\n",
        "                                    padding_vale=self.pad_idx)\n",
        "    data[\"captions\"] = captions\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRquY0WnC7jU"
      },
      "source": [
        "\n",
        "# Influenced by https://github.com/siddsrivastava/Image-captioning/blob/master/model.py\n",
        "\n",
        "class FlickrEncoderCNN:\n",
        "  def __init__(self, \n",
        "               embedding_dim, \n",
        "               do_freeze):\n",
        "    super().__init__()\n",
        "    # Loading resnet\n",
        "    resnet = torchvision.models.resnet18(pretrained=True)\n",
        "    fc_in_features = resnet.fc.in_features\n",
        "\n",
        "    # Defining layers\n",
        "    modules = list(resnet.children())[:-1]\n",
        "    self.resnet = nn.Sequential(*modules)\n",
        "    self.fc = nn.Linear(fc_in_features, embedding_dim)\n",
        "\n",
        "    # Freezing if needed\n",
        "    if do _freeze:\n",
        "      for param in self.resnet.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in self.fc.parameters():\n",
        "      param.requires_grad = True\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.resnet(x)\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class FlickrDecoderRNN(nn.Module):\n",
        "  def __init__(self, \n",
        "                vocab_size,\n",
        "                embedding_dim,\n",
        "                hidden_size,\n",
        "                padding_idx,\n",
        "                bidirectional=False,\n",
        "                dropout=0):\n",
        "    \n",
        "    super().__init__()\n",
        "    # Creating embeddings\n",
        "    self.embed = nn.Embedding(num_embeddings=len_vocab,\n",
        "                              embedding_dim=embedding_dim, \n",
        "                              padding_idx=padding_idx),\n",
        "                              scale_grad_by_freq=True,\n",
        "                              sparse=True,\n",
        "                              )\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                        hidden_size=hidden_size,\n",
        "                        batch_first=True,\n",
        "                        dropout=dropout\n",
        "                        bidirectional=bidirectional)\n",
        "    self.fc = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "  def forward(self, features, caption_seqs):\n",
        "      caption_seqs = caption_seqs[:,:-1] \n",
        "      embeddings = self.embed(caption_seqs)\n",
        "      total_input = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "      lstm_out, self.hidden = self.lstm(total_input)\n",
        "      outputs = self.fc(lstm_out)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "class FlickrRNN(nn.Module):\n",
        "  def __init__(self, \n",
        "               vocab_size,\n",
        "               embedding_dim,\n",
        "               hidden_size,\n",
        "               padding_idx,\n",
        "               bidirectional=False,\n",
        "               dropout=0,\n",
        "               freeze=True):\n",
        "    super().__init__()\n",
        "    self.encoder = FlickrEncoderCNN(embedding_dim, freeze)\n",
        "    self.decoder = FlickrDecoderRNN(vocab_size, \n",
        "                                    embedding_dim, \n",
        "                                    hidden_size, \n",
        "                                    padding_idx, \n",
        "                                    bidirectional, \n",
        "                                    dropout)\n",
        "  def forward(self, images, caption_seqs):\n",
        "    features = self.encoder(images)\n",
        "    outputs = self.decoder(features, caption_seqs)\n",
        "    outputs = nn.LogSoftmax(outputs)\n",
        "    return outputs\n",
        "  \n",
        "  def predict(self, images, states, max_len = 30):\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9BU36BiHWMf",
        "outputId": "77dae17b-ec81-47bd-eead-79626783b856"
      },
      "source": [
        "counter = collections.Counter(list(\"1123411233000009999\"))\n",
        "counter.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['1', '2', '3', '4', '0', '9'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}